{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "93406166",
      "metadata": {},
      "source": [
        "# Augmentation testing\n",
        "\n",
        "The best model was BeitLargePatch16 with validation accuracy 0.9856114983558655.\n",
        "- We test with the DEiT base model here (small and fast) in the interest of time\n",
        "\n",
        "Here we will experiment with image augmentation to see if this can be improved. If not, the extra complexity is not worth it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4af58b8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.applications import imagenet_utils\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "05e8cdd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_SIZE = 9367\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 8\n",
        "WORKERS = 4\n",
        "EPOCHS = 10\n",
        "\n",
        "BASE_PATH='../data'\n",
        "\n",
        "classes = [\n",
        "    'cup', \n",
        "    'fork', \n",
        "    'glass', \n",
        "    'knife', \n",
        "    'plate', \n",
        "    'spoon'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3669e2fd",
      "metadata": {},
      "source": [
        "First, we will load the training dataframe and split it into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d6dca2c5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>label</th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0560</td>\n",
              "      <td>glass</td>\n",
              "      <td>data/images/0560.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4675</td>\n",
              "      <td>cup</td>\n",
              "      <td>data/images/4675.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0875</td>\n",
              "      <td>glass</td>\n",
              "      <td>data/images/0875.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4436</td>\n",
              "      <td>spoon</td>\n",
              "      <td>data/images/4436.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8265</td>\n",
              "      <td>plate</td>\n",
              "      <td>data/images/8265.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Id  label              filename\n",
              "0  0560  glass  data/images/0560.jpg\n",
              "1  4675    cup  data/images/4675.jpg\n",
              "2  0875  glass  data/images/0875.jpg\n",
              "3  4436  spoon  data/images/4436.jpg\n",
              "4  8265  plate  data/images/8265.jpg"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train_full = pd.read_csv('data/train.csv', dtype={'Id': str})\n",
        "df_train_full['filename'] = 'data/images/' + df_train_full['Id'] + '.jpg'\n",
        "df_train_full.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "32713ffa",
      "metadata": {},
      "outputs": [],
      "source": [
        "val_cutoff = int(len(df_train_full) * 0.8)\n",
        "df_train = df_train_full[:val_cutoff]\n",
        "df_val = df_train_full[val_cutoff:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bbe4048e",
      "metadata": {},
      "source": [
        "## Baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4bfef22",
      "metadata": {},
      "source": [
        "Now let's create image generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2caa27c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# These models don't have the imagenet preprocessing built in so I have to apply this\n",
        "def preprocess_input(x, data_format=None):\n",
        "    return imagenet_utils.preprocess_input(\n",
        "        x, data_format=data_format, mode=\"tf\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c83ced9e",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4447 validated image filenames belonging to 6 classes.\n",
            "Found 1112 validated image filenames belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    dtype=\"float16\"\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    df_train,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    dtype=\"float16\"\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    df_val,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0f35ac7c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['cup', 'fork', 'glass', 'knife', 'plate', 'spoon'], dtype='<U5')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes = np.array(list(train_generator.class_indices.keys()))\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f82948f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_accuracy',\n",
        "    min_delta = 1e-4,\n",
        "    patience = 3,\n",
        "    mode = 'max',\n",
        "    restore_best_weights = True,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "callbacks = [earlystopping]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "416ccb4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_deit(model_url, res=IMAGE_SIZE, num_classes=len(classes)) -> tf.keras.Model:\n",
        "    inputs = tf.keras.Input((res, res, 3))\n",
        "    hub_module = hub.KerasLayer(model_url, trainable=False)\n",
        "\n",
        "    base_model_layers, _ = hub_module(inputs)   # Second output in the tuple is a dictionary containing attention scores.\n",
        "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(base_model_layers)\n",
        "    \n",
        "    return tf.keras.Model(inputs, outputs) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2a3d6e50",
      "metadata": {},
      "source": [
        "Warnings are normal; the pre-trained weights for the original classifications heads are not being skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e34cdac3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
          ]
        }
      ],
      "source": [
        "model_gcs_path = \"http://tfhub.dev/sayakpaul/deit_base_distilled_patch16_224_fe/1\"\n",
        "model = get_model_deit(model_gcs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7a7a80f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0e846ae7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "556/556 [==============================] - 58s 84ms/step - loss: 0.3040 - accuracy: 0.9375 - val_loss: 0.2098 - val_accuracy: 0.9523\n",
            "Epoch 2/10\n",
            "556/556 [==============================] - 45s 81ms/step - loss: 0.1726 - accuracy: 0.9703 - val_loss: 0.2560 - val_accuracy: 0.9631\n",
            "Epoch 3/10\n",
            "556/556 [==============================] - 45s 82ms/step - loss: 0.1265 - accuracy: 0.9744 - val_loss: 0.2648 - val_accuracy: 0.9748\n",
            "Epoch 4/10\n",
            "556/556 [==============================] - 45s 81ms/step - loss: 0.1228 - accuracy: 0.9784 - val_loss: 0.3421 - val_accuracy: 0.9559\n",
            "Epoch 5/10\n",
            "556/556 [==============================] - 45s 81ms/step - loss: 0.0903 - accuracy: 0.9827 - val_loss: 0.3940 - val_accuracy: 0.9523\n",
            "Epoch 6/10\n",
            "556/556 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9888Restoring model weights from the end of the best epoch: 3.\n",
            "556/556 [==============================] - 45s 81ms/step - loss: 0.0565 - accuracy: 0.9888 - val_loss: 0.4307 - val_accuracy: 0.9586\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    workers=WORKERS,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f779c6f2",
      "metadata": {},
      "source": [
        "## Augmented Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d128233f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4447 validated image filenames belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=90,\n",
        "    # width_shift_range=0.1,\n",
        "    # height_shift_range=0.1,\n",
        "    # shear_range=0.1,\n",
        "    # zoom_range=0.1,\n",
        "    vertical_flip=True,\n",
        "    horizontal_flip=True,\n",
        "    dtype=\"float16\",\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    df_train,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f718e557",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "299f6957",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "556/556 [==============================] - 49s 84ms/step - loss: 0.6728 - accuracy: 0.9193 - val_loss: 0.3386 - val_accuracy: 0.9604\n",
            "Epoch 2/10\n",
            "556/556 [==============================] - 45s 80ms/step - loss: 0.4564 - accuracy: 0.9440 - val_loss: 0.3417 - val_accuracy: 0.9640\n",
            "Epoch 3/10\n",
            "556/556 [==============================] - 45s 80ms/step - loss: 0.3932 - accuracy: 0.9503 - val_loss: 0.4703 - val_accuracy: 0.9496\n",
            "Epoch 4/10\n",
            "556/556 [==============================] - 44s 80ms/step - loss: 0.4090 - accuracy: 0.9487 - val_loss: 0.4106 - val_accuracy: 0.9568\n",
            "Epoch 5/10\n",
            "556/556 [==============================] - ETA: 0s - loss: 0.3251 - accuracy: 0.9573Restoring model weights from the end of the best epoch: 2.\n",
            "556/556 [==============================] - 45s 81ms/step - loss: 0.3251 - accuracy: 0.9573 - val_loss: 0.4285 - val_accuracy: 0.9586\n",
            "Epoch 5: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    workers=WORKERS,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90af692f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4447 validated image filenames belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    # rotation_range=10,\n",
        "    width_shift_range=0.25,\n",
        "    height_shift_range=0.25,\n",
        "    # shear_range=0.25,\n",
        "    # zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    dtype=\"float16\"\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    df_train,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed43118b",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = get_model_deit(model_gcs_path)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c716eb3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "556/556 [==============================] - 59s 86ms/step - loss: 0.9995 - accuracy: 0.9170 - val_loss: 0.5155 - val_accuracy: 0.9595\n",
            "Epoch 2/10\n",
            "556/556 [==============================] - 46s 82ms/step - loss: 0.4608 - accuracy: 0.9553 - val_loss: 0.4226 - val_accuracy: 0.9658\n",
            "Epoch 3/10\n",
            "556/556 [==============================] - 46s 82ms/step - loss: 0.4296 - accuracy: 0.9611 - val_loss: 0.4640 - val_accuracy: 0.9559\n",
            "Epoch 4/10\n",
            "556/556 [==============================] - 46s 82ms/step - loss: 0.3489 - accuracy: 0.9600 - val_loss: 0.5167 - val_accuracy: 0.9586\n",
            "Epoch 5/10\n",
            "556/556 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9622Restoring model weights from the end of the best epoch: 2.\n",
            "556/556 [==============================] - 46s 82ms/step - loss: 0.2775 - accuracy: 0.9622 - val_loss: 0.4374 - val_accuracy: 0.9604\n",
            "Epoch 5: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    workers=WORKERS,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "12306bea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4447 validated image filenames belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=45,\n",
        "    # width_shift_range=0.1,\n",
        "    # height_shift_range=0.1,\n",
        "    # shear_range=0.25,\n",
        "    # zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    df_train,\n",
        "    x_col='filename',\n",
        "    y_col='label',\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e31dbaf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = get_model_deit(model_gcs_path)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fddf42df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "556/556 [==============================] - 54s 83ms/step - loss: 0.9347 - accuracy: 0.9249 - val_loss: 0.4307 - val_accuracy: 0.9649\n",
            "Epoch 2/10\n",
            "556/556 [==============================] - 44s 80ms/step - loss: 0.4290 - accuracy: 0.9591 - val_loss: 0.7858 - val_accuracy: 0.9344\n",
            "Epoch 3/10\n",
            "556/556 [==============================] - 45s 80ms/step - loss: 0.2410 - accuracy: 0.9696 - val_loss: 0.3871 - val_accuracy: 0.9694\n",
            "Epoch 4/10\n",
            "556/556 [==============================] - 45s 80ms/step - loss: 0.2794 - accuracy: 0.9692 - val_loss: 0.3911 - val_accuracy: 0.9640\n",
            "Epoch 5/10\n",
            "556/556 [==============================] - 44s 80ms/step - loss: 0.2748 - accuracy: 0.9687 - val_loss: 0.5312 - val_accuracy: 0.9523\n",
            "Epoch 6/10\n",
            "556/556 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9746Restoring model weights from the end of the best epoch: 3.\n",
            "556/556 [==============================] - 45s 80ms/step - loss: 0.2116 - accuracy: 0.9746 - val_loss: 0.5882 - val_accuracy: 0.9631\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    workers=WORKERS,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f351cb12",
      "metadata": {},
      "source": [
        "Very little to be gained from image augmentation for this dataset it seems. We will forgo augmentation as the additional complexity does not come with significant benefit "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py-39-tf-2.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "f55e75aed3a0cf5024464fbc8e31675ba942646c9cd79dbaff3424bb49495c78"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
